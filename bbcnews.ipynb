{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\AppData\\Local\\Temp\\ipykernel_19496\\813672170.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests \n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "import schedule\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code():\n",
    "    bbc_url = \"https://www.bbc.com/\"\n",
    "    html = requests.get(bbc_url)\n",
    "    bsobj = soup(html.content, features=\"lxml\")\n",
    "    bsobj\n",
    "\n",
    "    # Fetching Headings\n",
    "    complete_news_one_go = []\n",
    "    for headings in bsobj.findAll('h3', {'class': 'media__title'}):\n",
    "        complete_news_one_go.append(headings.text)\n",
    "\n",
    "    for headings in bsobj.findAll('a', {'class': 'reel__link'}):\n",
    "        complete_news_one_go.append(headings.h3.text)\n",
    "\n",
    "    for headings in bsobj.findAll('a', {'class': 'top-list-item__link'}):\n",
    "        complete_news_one_go.append(headings.h3.text)\n",
    "\n",
    "\n",
    "    # Removing Extra Characters\n",
    "    char = '\\n'\n",
    "    for idx, ele in enumerate(complete_news_one_go):\n",
    "        complete_news_one_go[idx] = ele.replace(char, '')\n",
    "        \n",
    "    complete_news_one_go = [x.strip() for x in complete_news_one_go]\n",
    "\n",
    "\n",
    "    # Fetching the Heading Links\n",
    "    complete_news_one_go_links = []\n",
    "    for headings in bsobj.findAll('h3', {'class': 'media__title'}):\n",
    "        if headings.a is not None:\n",
    "            complete_news_one_go_links.append(headings.a['href'])\n",
    "\n",
    "    for headings in bsobj.findAll('a', {'class': 'reel__link'}):\n",
    "        if headings is not None:\n",
    "            complete_news_one_go_links.append(headings['href'])\n",
    "\n",
    "    for headings in bsobj.findAll('a', {'class': 'top-list-item__link'}):\n",
    "        if headings is not None:\n",
    "            complete_news_one_go_links.append(headings['href'])\n",
    "\n",
    "    address = \"https://www.bbc.com\"\n",
    "    complete_news_one_go_links = [address + x if not x.startswith(address) else x for x in complete_news_one_go_links]\n",
    "\n",
    "    # Fetching the Description\n",
    "    complete_news_one_go_desc = []\n",
    "    for link in complete_news_one_go_links:\n",
    "        page = requests.get(link)\n",
    "        bsobjtwo = soup(page.content)\n",
    "        for news in bsobjtwo.findAll('article',{'class':'ssrcss-pv1rh6-ArticleWrapper'}):\n",
    "            complete_news_one_go_desc.append(news.p.text.strip())\n",
    "\n",
    "    \n",
    "    # Complete Extra News\n",
    "    finalized_news_one_go = dict(zip(complete_news_one_go, complete_news_one_go_desc))\n",
    "    # finalized_news_one_go\n",
    "\n",
    "    # Convert the dictionary into a DataFrame, using the keys as the index and the values as the column\n",
    "    df = pd.DataFrame.from_dict(finalized_news_one_go, orient=\"index\", columns=[\"Description\"])\n",
    "    date_today = date.today()\n",
    "    df.to_csv(\"{}.csv\".format(date_today), index_label=\"Headlines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.every(60).minutes.do(run_code)\n",
    "# Keep the program running\n",
    "while True:\n",
    "    # Run any pending tasks\n",
    "    schedule.run_pending()\n",
    "    # Wait for one second\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Data Into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.NewsScrapping # use your database name\n",
    "collection = db.news # use your collection name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.insert_one(finalized_news_one_go)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
